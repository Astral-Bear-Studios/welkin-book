% SPDX-FileCopyrightText: 2023 Oscar Bender-Stone <oscarbenderstone@gmail.com>
% SPDX-License-Identifier: CC-BY-4.0
% 4-spec.tex - Specifices the Welkin language and provides a theoretical framework
% for general implementations

\chapter{The Welkin Standard}

% TODO: create a version for welkin-standard.tex that directly includes the copyright notice
% Henceforth, compliant Welkin implementations will be collectively referred to as ``Welkin.'' A formal definition of
This standard requires a cursory background in discrete mathematics and Context Free Grammars. A reading of [] and [] suffices to understand this document.

\section{Preliminaries}

% Helpful macros for terms for character encoding in math mode
\newcommand*{\chars}{\mathrm{CHAR}}
\newcommand*{\numbers}{\mathrm{NUMBER}}
\newcommand*{\whitespaces}{\mathrm{WHITE\_SPACES}}
\newcommand*{\reserved}{\mathrm{RESERVED}}
\newcommand*{\strings}{\mathrm{STRING}}
\newcommand*{\term}{\mathrm{term}}
\newcommand*{\terms}{\mathrm{terms}}
\newcommand*{\delimiters}{\mathrm{DELIMITERS}}
\newcommand*{\encoding}{\mathcal{E}}
\newcommand*{\decoding}{\mathcal{D}}

\newcommand*{\scope}{\textrm{scope}}

\subsection{Character Encodings}
% TODO: generalize to byte encodings and numbers.
In a formalist fashion, we define text, character encodings, and character decodings as generalized notions. The discussion here may be carried out with bytes and specific data formats, but these concepts are beyond the scope of this standard.

Let Char be an arbitrary, finite set. An \textbf{encoding} is an injective mapping $\encoding : \mathbb{N} \to \textrm{Char}.$ The associated \textbf{decoding} is the left-inverse $\decoding: \mathrm{Char} \to \mathbb{N}$ of $\encoding.$ There is a natural extension $\decoding^{*}: \textrm{Char}^{*} \to \mathbb{N}^{*}$
that maps sequences in Char pointwise to sequences in $\mathbb{N}^{*}.$

Character encodings may be given as finite tables, matching natural numbers with characters. Several major encodings are formally defined in the following sources.
\begin{itemize}
	\item ASCII []
	\item UTF-8 []
	\item UTF-16 []
\end{itemize}

We denote $\chars = \decoding(\chars)$ and CHAR* as the
Kleene-closure of CHAR, whose elements are called \textbf{words.}
\footnote{Traditionally, the Klenne-closure of a set $A$ is denoted by $A^{*}.$ However, to ensure our BNF
can be written in pure ASCII, we append $*$ without a superscript on $\chars$ and its subsets.}
We recognize distinguished (possibly empty) subimages $\numbers, \whitespaces \subset \chars,$ as well as a set $\delimiters \subset \chars^{2}.$ A $\textbf{string}$ $s$ is a word such of the form
\begin{align*}
s = d_{1}ud_{2}, u \in \chars \setminus \delimiters, (d_{1}, d_{2}) \in \chars^{2}.
\end{align*}
We call $u$ the \textbf{contents} of $s. $We define $\strings := \strings(\delimiters)$ as the set of strings over $\delimiters.$

Every standard Welkin grammar is written in ASCII, but the interpreter may support any additional character encoding. (See Section ?.?).

As an auxiliary set, define
\begin{align*}
	\strings &:= \strings(\delimiters) \\ &= \{s \in \chars* \;|\; s = d_{1} u d_{2}, u \in (\chars\} \setminus \delimiters)^{*}, d_{1}, d_{2} \in \delimiters\},\end{align*}
where $\delimiters \subset \chars^{2}.$
Strings may include their delimiters by escaping them, i.e., using a sufficient prefix or suffix DELIMITER\_ESCAPE to distinguish them from delimtiers.

We implicitly assume that $CHAR*$ does not conflict with literals defined in a given standard grammar. In terms of the recommended LALR parser, this means that literals are matched first, not identifiers containing those characters. However, this can be changed by creating a custom grammar (see Section ?).


\subsection{EBNF Variant}
Our variant of EBNF uses the notation in Table ?.?.
\begin{center}
\begin{tabular}{ | m{3cm} | m{5cm} | m{5cm} | }
	\hline
  Concept & Notation & Example \\
  \hline
	Rule Assignment & $=$ & $term = atom$\\
  Empty String & $\varepsilon := \emptyset \in \chars*$ & $term = \varepsilon$ \\
  Concatenation (No white space) & ``.'' & \\
	Concatenation (white space) & Separated by whitespace & $term = A B$ \\
	Alternation & $|$ & $term = A | B$\\
	Zero or more instance & $*$ & $terms = term*,$ equivalent to $terms = term terms | term | \varepsilon$ \\
	One of more instance & $+$ & $terms = term*,$ equivalent to $terms = term terms$ \\
  Literal characters & ``chars'', with $chars \in \chars*$ & $list = "[" terms "]"$ \\
  \hline
\end{tabular}
\end{center}
Each BNF has an associated subset $\reserved \subseteq \chars*$ for any literals that appear in the grammar. We will explicty state these for standard Welkin grammars in the next section.
% TODO: convert this into a table for easy access
% \begin{itemize}
%   \item $::=$ denotes rule assignment,
%   \item \texttt{term ::= $S \subseteq \chars^{*}$} means \texttt{term $\in S$},
%   \item $|$ denotes alternation,
% 	\item In a given choice, an arrow $\to$ denotes a new rule name. For example, the rule
% 		\begin{bnfgrammar}
% 			term ::= `A' $\to$ A | `B' $\to$ B
% 		\end{bnfgrammar}
% 		is equivalent to
% 		\begin{bnfgrammar}
% 			term ::= A || B ;;
% 			A ::= `A' ;;
% 			B ::= `B'
% 		\end{bnfgrammar}
% 	\item \texttt{term*} means 0 or more instances and is shorthand for
% 	\begin{center}
% 		\texttt{terms ::= term terms | term | $\varepsilon$},
% 	\end{center}
% 	\item \texttt{term+} means 1 or most one instances and is shorthand for
% 	\begin{center}
% 		\texttt{terms ::= term terms | term},
% 	\end{center}
%   \item Elements of CHAR* are included in quotes. To avoid confusion, literal quotes are denoted with ['].
% 	\item A production between a terminal and non-terminal, such as $term"A"$, means there is no whitespace character that appears between them. All other productions, in the form \texttt{term1 term2} any number of whitespaces may be included.
% \end{itemize}
\section{The Welkin Language}

There are three fundamental variants of Welkin that define the foundation for the language:
\begin{itemize}
	\item Base Welkin, mirroring the key properties of the core data structure,
	\item Attribute Welkin, extending Base Welkin with attributes. Attributes are a limited type of directive that can customize how the interpreter accepts or presents data,
	\item Binder Welkin, enabling arbitrary evaluation of Welkin files and access to the Operating System. This is equivalent to Attribute Welkin with two new directives: $@eval$ and $@exec.$ See Section ?.? for more details.
\end{itemize}
Each of these variants can be parsed with LALR parsers and fundamentally have the same semantics. However, in Binder Welkin, $@eval$ makes the interpreter Turing complete (see Section ?.?), and using $@exec$ can significantly impact the user's system. For this reason, Binder Welkin is a separate, optional component, as detailed in the Section ?.?
% \begin{itemize}
% 	% \item First Define character encodings in general. Helpful reference: \url{https://www.w3.org/International/questions/qa-what-is-encoding}
% 	%       \begin{itemize}
% 	% 	      \item For wide spread use, there should be different character encodings used for \textit{direct comparison} with Welkin files. Ultimately, every Welkin file will be converted into a standard binary (or possibly text) file to store the object
% 	%       \end{itemize}
% 	\item Determine a suitable BNF for Welkin, which can be parsed with LALR (or otherwise a more efficient parser)
% 	      \begin{itemize}
% 		      \item Key goal: make Welkin's syntax fully decidable and efficient to parse. An important component of CFLT called the Semantics Lifting Lemma (TBD) essentially says we can embed a complex syntax into a semantics. (This proof will hopefully be constructive and work for any random syntax, no matter how crazy it might be). In other words, using an efficient parser does NOT limit how expressive Welkin is.
% 		      \item Presuming the result above, there will be two variants: the finite (regex) and full versions.
% 		            \begin{itemize}
% 			            \item The finite, or regex, version is purely for regex-definable files.
% 			            \item The full version will be LALR parsed, as it is generally a standard for programming languages. Not only is it efficient (both in speed and memory), but any grammar written in LALR is unambiguous (reference needed!).
% 			            \item Now that the idea of these two versions is solidifed, we need some common terms. Most of these should come from graphviz, but also in other note taking formats.
% 		            \end{itemize}
% 		      \item The standard format should read just like an ordinary programming language. It may be akin to graphviz, but it should prioritze on the contents of each node and edge, not necessarially how they are rendered. (A better thought would be to put rendering information in a standard \textit{library}, which could then be minimized when browsing through a Welkin file/project.)
% 		            \begin{itemize}
% 			            \item Welkin essentially needs the key elements from set theory: conjunction, disjunction, negation, implication, etc. We can use corresponding symbols for these: $\&\&, ||, \neg, \rightarrow$. In \textit{customizable files}, these symbols can be overloaded and added upon.
% 			            \item Key goal: make this FULLY compatible with dot. (In fact, for a prototype, we can work with dot directly, but we should make it helpful for our needs).
% 									\item Another important point: we want to say that graph ALWAYS refers to a metagraph (to avoid redundancy)		            \end{itemize}
% 	      \end{itemize}
% 	\item Following CFLT, explain a suitable semantics for Welkin.
% 	      \begin{itemize}
% 		      \item We need to determine how to implement all of the axioms.
% 		      \item We also need to use a suitable proof system (e.g., Hilbert, Gentzen, etc.). Maybe that could be decided in CFLT?
% 	      \end{itemize}
% \end{itemize}
% TODO: figure out how to handle references to self. Is a separate keyword 'self' needed?
%\renewcommand{\syntleft}{\normalfont\bfseries}
%\renewcommand{\syntright}{}

% TODO: directly convert this into other grammars, such as lark.
% There should be some consistent procedure to ensure that the BNF here
% is the same as those found in any implementation
% Interesting idea: when there is a node A that should connect to other nodes B_1, B_2, ..., B_n,
% we require that the latter nodes be wrapped in their own graph. That way, we can stay consistent with
% GraphViz notation (for possible compatiblity reasons), but at the same time, we can keep track of the out-neighbors (out going
%\renewcommand{\bnfexpr}{\textbf}
\subsubsection*{Syntax}
Set $\reserved = \{\texttt{\{, \}, ., -, ->, <-}\}.$
% \SetBNFConfig{

%   relation-sym-map = {
%     {::=} = {\ensuremath{=}},
%   }
% }
\begin{center}
  \begin{tabular}{ | c | c | c |}
	Variant & Grammar & Notes \\
	Base Welkin &
  \begin{bnf}
  terms ::= term*;;
  graph ::= unit? `\{' terms `\}' ;;
  connections ::= term (connector term)+ ;;
	connector ::=

   | `-' term `-' $\to$ edge
   | `-' term `-\>' $\to$ left\_arrow
	 | `<-' term `-' $\to$ right\_arrow ;;
  member ::= unit? (`.'(ident || string)? || `\#'num )+ ;;
	unit ::= ident || string || num ;;
  ident ::= CHAR\* ;;
	string ::= STRING ;;
	num ::= NUMBER
\end{bnf}
& If $\numbers = \emptyset,$ any instance of \texttt{num} should be removed from the parser. \\

\end{tabular}
\end{center}
% relation-sym-map = {
%       {::=} = {\ensuremath{=}}
%	}


	% term ::= graph || connections || member || unit ;;
	% graph ::= unit? `\{' terms `\}' ;;
	% connections ::= term (connector term)+ ;;
	% connector ::= `-' term `-' $\to$ edge
	% | `-' term `->' $\to$ left\_arrow
	% | `<-' term `-' $\to$ right\_arrow ;;
	% member ::= unit? (`.'(ident || string)? || `\#'num )+ ;;
	% unit ::= ident || string || num ;;
	% ident ::= CHAR* ;;
	% string ::= STRING ;;
	% num ::= NUMBER



% TODO: the slashes quotes above should be slashes, and then quotes. We need to fix this!
Base Welkin is given by the grammar in Figure ??. Note that if $\numbers= \emptyset,$ any instances of the rule \texttt{num} must be excluded from the parser. This grammar defines the text-based interface to the Welkin Information Graph (see Subsection 2.? for more details). Recall Rule ?.? that specifices which characters are forbidden in identifiers.
\\ Throughout this document, Welkin documents are formatted with the following convention: the ASCII sequence \texttt{->} is rendered as $\to$ (A graphical user interface may support this rendering via glyphs). % TODO: put special renderings in a table
Attribute Welkin is given in Figure ?.?
% \begin{bnfgrammar}
%   statement ::= (directive || construct || term)* ;;
%   directive ::= `@'.attribute ;;
%   attribute ::= `import' tuple $\to$ import
%   | test ;;
%   construct ::= operator || tuple || list ;;
%   operator ::= ;;
%   tuple ::= ;;
%   list ::=
%  \end{bnfgrammar}
  % TODO: figure out suitable grammar composition notation
Finally, Binder Welkin is given by the BNF in Attribute Welkin composed by two new directives. In BNF, these are appended to the directives rule:
\begin{itemize}
	\item \texttt{eval ::= `eval'.`(' unit `)'},
	\item \texttt{exec ::= `exec'.`(' string `)'}.
\end{itemize}
We explain the semantics for these directives in Section ?.?

% TODO: recognize, in cflt, that the above welkin file is in fact a context free grammar! The more important part, which we need to still define, is the semantics, which will have its full strength with the full grammar. (Maybe we should change that option to be semantics instead?)

\subsubsection*{Semantics}
We break down our semantics first by terms. Directives are handled separately in the next section.
\begin{definition}
Equality of terms.
\begin{itemize}
  \item \textbf{Basis.} Two units are equal if they are the same kind and obey one of the following.
	\begin{itemize}
	  \item \texttt{ident} terms are equal if their corresponding characters are equal,
      \item \texttt{string} terms are equal if their corresponding contents are equal. Thus, \texttt{``A''} coincides with \texttt{'A'},
	  \item \texttt{num} terms are equal if they represent the same value. Thus, \texttt{1} coincides with \texttt{10E}.
	\end{itemize}
  \item \textbf{Recursion.}
		\begin{itemize}
		  \item Equality of connectors
				\begin{itemize}
				  \item
				  \item
				  \item \texttt{edge} terms are equal if they correspond as both left and right arrows.
				\end{itemize}

		  \item Two connections are equal if they connect the same terms and have equal connectors.
		  \item Two graphs are equal if they contain the same terms.
	\end{itemize}

\item
\end{itemize}
\end{definition}
A \textbf{scope} is recursively defined and intutively is a level of terms.
\begin{definition} (Scope)
 Let $t$ be a term.
  \begin{itemize}
	\item If $t$ is not contained in a graph, then $\scope(\texttt{term}) = 0,$
	\item If $G' \in G$ are both graphs and $\scope(G') = n,$ then for all $t \in G',$ $\scope(t) = \scope(G) + 1.$
\end{itemize}
\end{definition}
A \textbf{valid} base Welkin file consists satisfies a unique naming rule: in every scope, there are no name collisons. In particular, every graph must \textbf{only be defined once.} Note that, by the way equality was defined between two numbers,
  there can only be one representation of a given number in a scope. For example, using $1$ and $10*E^{-1}$ in the same scope would produce a name collison.
\\ We first form an Abstract Syntax Tree (AST), from which we form the final stored data in a \textbf{Welkin Information Graph.}
\begin{definition}
  Base Welkin is parsed into the following AST $\mathcal{A}.$
  \begin{itemize}
	\item Every term is a new subtree with its contents as children.
    \item Every graph is an ordered pair of its aliases and list of children.
    \item Every connection is an ordered pair:
		  \begin{itemize}
			\item Left arrows $u - a \to v$ correspond to a triple $(u, a, v);$
			\item Right arrows $u \leftarrow a - v$ correspond to the triple $(v, a, u);$
			\item Edges correspond to both a left and right arrow.
		  \end{itemize}
	\item Every unit is converted into its corresponding encoding via $\encoding^{*}.$
  \end{itemize}
 \end{definition}
% TODO: talk about encoding of numbers. This probably a separate encoding from the one used to write the welkin file
\begin{definition}
	A \textbf{Welkin Information Graph (WIG)} $\mathcal{G}$ consists of
  \begin{itemize}
    \item sets $G_{0}, G_{1}, \cdots, G_{n}$ of \textbf{layers},
    \item for each $0 \leq k \leq n,$ functions $s_{k}, t_{k}: G_{k+1} \to G_{k}$ called the $i$-th $\textbf{source}$ and $\textbf{target}$ maps, respectively,
    \item for each $k,$ an injective function $i_{k}: G_{k} \to G_{k+1},$
    \item a set $\mathcal{L}$ of \textbf{labels} or \textbf{aliases}, and
    \item for each $k,$ an injective function $l_{k}: \mathcal{L} \to G_{k}$ called the $k$-th \textbf{labeling map,}
  \end{itemize}
  that obey the following equations:
  \begin{itemize}
    \item $s_{k} \circ s_{k+1} = s_{k} \circ t_{k+1}$
    \item $t_{k} \circ s_{k+1} = t_{k} \circ t_{k+1}$
    \item $s_{k} \circ i_{k} = \textrm{id} = t_{k} \circ i_{k} $
    \item (Condition to allow edges to be the ``intervals'' of connectors)
  \end{itemize}
  We may illustrate the above definition via the following diagram.
\end{definition}
% Cite: WIGs are a special form of reflexive n-graphs, or n-globular sets. We add the labeling function to
% store a record of labels from a previous Welkin file, and the last property is used to distinguish internal arcs from
% connectors
Notice that not every vertex or edge in a WIG have an alias, as opposed to a colored graph. There are several examples demonstrating that, under a suitable transformation, a normalized WIG may contain new structures not found in a Welkin file. See Example ??.

To select a single graph from a layer, we use the following definition.
\begin{definition} (Abstract version of graphs) ...
\end{definition}
\begin{lemma}
The conversion from ASTs to Welkin Information Graphs is valid.
\end{lemma}
The final output of parsing is a normalized WIG. We define Welkin Canonical Form in the following fashion.
\begin{definition}
A WIG is in \textbf{Welkin Canonical Form (WCF)} if ...
\end{definition}
Based on this form, we have chosen a unique way to represent Welkin files. In particular, there is a representation under WIG (generalized) homotopies. We prove that there is a polynomial (or exponential?) algorithm to convert any WIG into WNF.

\section{General Application Behavior}

Note that all apparent structures may be adjusted under cryptomorphism.
\subsubsection*{Directives}
Each directive relies on the following components.
\begin{itemize}
  \item Parser: takes in a Welkin file and generates an AST,
  \item Validator: ensures that the AST is valid, raising an error that directly points to a violation,
  \item From here, an AST may be processed by three different means:
		\begin{itemize}
		  \item Recorder: takes the AST, converts it into a WIG in WCF, serializes the data,
		  \item Attributor: %TODO: change name!
		  \item Binder:
		\end{itemize}
\end{itemize}
\begin{center}
  \begin{tabular}{| c | c | c |}
	Directive & Definition & Example \\
	\hline
	\texttt{import} & Concatenates the file & Example

  \end{tabular}
\end{center}




\subsection{Customization}
All Welkin files are infinitely customizable via the welkin config file, which is written in attribute welkin.

Every .welkin file has a corresponding configuration. It may either be put as a comment or (preferred) written in a separate file, which, by default, is called config.welkin. It has the following format:
% TODO: replace itemzie with listing
% TODO: decide how to work with a folder (or folders) of config files
\begin{itemize}
	\item Encoding
				\begin{itemize}
					% TODO: list major ascii versions/varieties. Need an official reference for this!
					\item Options: ascii, utf-8, utf-16, other
					\item In the case of other: we need to specify how to define an encoding. (We need a light-weight API for implementations)
				\end{itemize}
	\item Grammar
				\begin{itemize}
					\item Strength: bounded (only finitely nested graphs with a given nesting limit, no recursion), no-self (arbitrary nesting limit, but no recursion), full (recursion allowed)
					\item Customized: use a builtin template or custom welkin file. These can be used to change any part of the grammar, including adding keywords, the symbols used, adding new symbols, etc. Essentially, this will be a way to built new grammars from the original specification; we will need a separate parser for this (i.e., a parser of BNF/Welkin accepted notation).
				\end{itemize}

	\item (Optional) Language
				\begin{itemize}
					\item Defaults to English. Can be written in the writer's desired language (as long as it has been configured in Encoding above)
				\end{itemize}
\end{itemize}

Alternatively, a file with a different name may be used; see th


% \begin{itemize}
% 	\item Current problem: we need a way to recognzie which Welkin modules (graphs) define grammars. (Then we can figure out how one can customize \textit{any} aspect of Welkin). Here are some possible solutions (and evaluations thereof):
% 				\begin{itemize}
% 					\item Introduce a new meta-arrow \texttt{=>} designed specifically to be detected by Welkin (the left hand side denotes any terms in the user file; the right hand side must be terms that are defined by the Welkin interface). Using this symbol may initially work, but what if a user would like to use this meta-arrow in their grammar? How do we control how much the meta-arrow is used? I feel as though this could be dangerous, if not managed. Moreover, logistically, \textit{how} do we inform users of the terms they may bind to? For example, in the long-term, how do we make it clear that there is a ident for, say, num (standing in for general int/float)?
% 					\item Create a different welkin file, such as welkin.config, that only uses minimal syntax (or a slightly extended version). This would ensure that there is a distinction between the information in welkin files and the grammars that they utilize. However, we want to makek Welkin infinitely customizable, so I aim not to use this solution, if at all possible.
% 					\item Preferred, but still in the works: bootstrap interfaces. We would probably want to add this subsection later on, but definitely mention it while customizing grammars. \textit{Actually, customizing grammars is NOT a part of the standard, but rather, it is a part of the API. We will include EBNF and the custom grammar for standard Welkin}
% 				  \item I am aiming to use the last solution, but what should the standard Welkin language be? Should it be aimed for programmers? Should it be aimed for general information preservation?
% 				\end{itemize}
% 	\item At this point, I believe I am aiming to use the first solution; it would be benefitcial for Welkin itself to intrepret what Welkin files do, not \textit{directly} the other way around. At its core, Welkin files store information; it is up to an external program (such as Welkin) to figure out \textit{how} those files may be interpreted.
% 				\begin{itemize}
% 					\item Moreover, doing the other way around directly would prove formally difficult. For correctness, we would have to worry about having a correct model for C, the operating system, etc.. But, part of this project is building the infrastructure for \textit{getting and organizing} that specification! For initial implementations, Welkin need not \textit{have} a low-level specification, so neither should this standard. That should be taken care of in a formally verified programming language (or formally verified binaries).
% 					\item I hope that using the meta arrow could be optional in time; there could easily be ambigiuity if
% 								care is not taken. For example, the rule (test1 -> test2 -> graph) is ambiguous because of the ordering of the parantheses; it is unclear what is ultimately turned into a rule.
% 					\item However, if the lefthand side only uses terms in the minimal grammar, this would not be ambigiuous. That could certainly work. Regardless, \textit{we need to make it clear HOW Welkin, the program, is involved. Using the meta arrow is probably the least intrustive way to go about this, and can easily be customized by the user.}
% 				\end{itemize}

%   \item Rephrasing above problem: how do we make it clear when we are defining a new semantics?
% 		\begin{itemize}
% 		  \item A key realization: Welkin \textit{is a semantics language.} You can define anything in it! It suffices to show that we can embed Welkin into discrete foci (or, more directly, can define any Chomsky grammars. This part is pretty clear from the semantics given to base welkin (or for graphs, and for connections, not for a reference to self + ?))
% 		  \item We could try making code blocks that specifically contain bindings. This would be a fancier alternative to restricting names (or coming up with a system, such as names prefixed with $_{}$). My hesitance to go with this solution is flexibility; should we force the users to do this?
% 		  \item Here is a possible question we can ask: how do we distinguish between regular and \textit{interfaced} Welkin?
% 		  \item The goal is to have the \textit{guest} tackle how to interpret Welkin... but we still want to put the guest in Welkin!
% 				\begin{itemize}
% 				  \item How do we gain ``awareness'' about Welkin's interfaces?
% 				  \item We need a place to talk about \textit{interpretations.} We know Welkin can talk about it, but how do we \textbf{reference external things? (Files, programs, even a simple microcontroller?)}
% 						\begin{itemize}
% 						  \item Do we directly include a pointer to the file? Is that strictly necessary8? It could definitely break on different machines. We would have to keep some config data in mind.
% 						  \item We know that any possible import symbols, as per compiler theory (and general humanb comprehension) need to be applied directlyat the \textit{top} of a page. But how do we define the import statement itself?
% 						  \item Can we bootstrap an interface mechanism?
% 						\end{itemize}
% 				\end{itemize}
% 		\end{itemize}

%   \item Current (recommended) solution: we define standard Welkin to have direct access to the interpreter, similar to prolog. \textit{Later on, we will make it clear that standard Welkin can be queries, but can do more.} Ultimately, \textit{it is up to the individual to decide whether they include programming elements into their own welkin files. This is the key thing! } All the user needs to do is define their EBNF in standard Welkin
% 		\begin{itemize}
% 		  \item Here's a big idea: we can go back and \textit{refine} the base interface through standard Welkin! That's the powerful of Welkin, after all!
% 		  \item More precisely, we need to implement Vero, the Welkin interface for stating and verifying formal properties. \textit{We only want to make this through the APIs specified by Welkin. The implementation should NEVER matter for verification.}
% 		  \item There should be a separate document outling the thought process behind Vero, but for the most part, Vero should be defined \textit{with Welkin alone.} It will be the big first test for Welkin to make sure Welkin's claim for any layer of abstraction works well.
% 		\item Vero comes with its own grammar which is fully customizable (some mathematicians/logicians/etc may want to change it, for their personal preference). We also have certain semantics that impose checks on graphs. This is the key thing here: \textit{we want to make checks/formal properties explicit.}
% 		  \item Main takeaway: \textit{the interface can always be defined first. It need not be perfect. We can start with any interface and clarify it with Welkin. We can delete things, but we NEVER have to. We can add onto it instead}
% 		  \item Moreover, using standard Welkin for \textit{all implementations} is a good sanity check; how can we check different configs with completely different grammars! At some point, \textit{ we need to use the same language to customize things!} Starting with this idea, it is straight forward to make custom interpreter settings \textit{in your own language.} You just have to invoke it in standard welkin at some point.
% 				\begin{itemize}
% 				  \item In fact, some core plugins will use this idea as well! One of the big things to have is a suitable \textit{build system.} That will be included and have a straightforward interface.
% 				  \item Other key starter grammars include: bullets (for bullet point type notes) and literate (for a starting point with code blocks). (As an implementation detail, these should be \textit{optional} packages the user can be install; they should be more straightforward to get setup)
% 				\end{itemize}
% 		\end{itemize}


% \end{itemize}

\subsubsection*{Attribute Welkin}

In Welkin, we informally write the BNF above as follows:
% TODO: explain unit notation (as it maybe clearer than the recursion below). In other words, mark arbitrary variables with the keyword unit
% TODO: decide whether to introduce another arrow symbol for custom grammars.
% While we have imposed few to no restrictions on custom grammars (besides being LALR), it may be the case
% that multiple people want to use => for their own purpose. Is there a convenient way we can do this?
\begin{quote}{\ttfamily \raggedright \noindent
	term -> \{ graph connection ident string\}\\
	graph -> \{\{ident \{\}\}->`\{'--term--`\}'\}\\
	connection -> \{term--connector--term\}\\
	connector -> \{edge arrow\}\\
	edge -> `--'\\
	arrow -> `->'\\
	ident -> CHAR*\\
	string -> ``'' CHAR* ``'' | `\`' CHAR* `\''
}\end{quote}

\section{Core Algorithms}

\subsection{Graph Encoding}



\label{ch:spec}
